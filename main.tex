\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{HPTCDL}{'14 New Orleans, Louisiana USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Operator polymorphism for distributed computing in Julia}

\numberofauthors{2}

\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Jiahao Chen\\
       \affaddr{Massachussetts Institute of Technology}\\
       \affaddr{Computer Science and Artificial Intelligence Laboratory}\\
       \affaddr{77 Massachusetts Avenue}\\
       \affaddr{Cambridge, Massachusetts 02139, USA}\\
       \email{jiahao@mit.edu}\\
% 2nd. author
\alignauthor
Alan Edelman\\
       \affaddr{Massachussetts Institute of Technology}\\
       \affaddr{Department of Mathematics and Computer Science and Artificial Intelligence Laboratory}\\
       \affaddr{77 Massachusetts Avenue}\\
       \affaddr{Cambridge, Massachusetts 02139, USA}\\
       \email{edelman@mit.edu}
}

\date{15 October 2014}

\maketitle
\begin{abstract}
High level languages provide nice abstractions for most computing tasks, but fall short of providing useful abstractions for parallel computing.

The lack of useful abstractions poses significant challenges for users of parallel computing.

In this paper we study a primitive parallel algorithm, namely that of prefix reduction, and show how it can be implemented with operator overloading so that the parallelism occurs at the operational level, not the algorithmic level.

The ability to write such code shows that Julia's abstractions are useful for reasoning about the structure of parallel algorithms by successfully abstracting away implementation details.

Code reuse for other tasks as well like visualization.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Theory}

\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
Nobody really understands how to do parallel computing. In practice, a lot of parallel computing code is mucky and gross because you have to embed all sorts of low level MPI initialization and communication primitives in your code.

Here is a typical fragment of code that uses MPI

How do the GPU Gems chapter showcase parallel prefix?

Can we do better to abstract away the low level communication protocols of a distributed algorithm?

In Julia we expose how overloading at the operator level parallelism can be used to showcase the essentials of what is going on while relegating the parallelism to a lower more primitive level. In other words, successful abstraction!

\section{The Julia language}

Type system and multiple dispatch. The type system is a resource for programmers, not just a low level compiler system that is hiden from the user. Being able to use types in user written code has turned out to be a great boon for writing technical code.

Polymorphism. Julia provides two distinct kinds of polymorphism. One is the paradigm of multimethods and the other is parametric polymorphism. We will focus more on how multimethods are helpful.

\section{The prefix reduction algorithm}

What is prefix reduction? Making use of associativity (or approximate associativity side node about floating point and how it doesn't really matter for most applications)  to regroup operations to provide different execution strategies.

Examples of things we can do prefix reduction for. Many examples, but to just name a few relevant for technical computing, we can do 

- matrix multiplication

- solving recurrences

- ??? (one more)

Here is a code listing for the special case of 8

It's nice that we can use Unicode

It's nice that we can use infix notation

It's nice that the operator is passed as an argument the function. This means that we have just written a higher order function (or functional).

Here is a picture showing what happens

Here is the general case for serial reduction

Here is the tree branching version in the general case

This is all serial code and all we have done is to do more work. So far. The point is that this algorithm is parallelizable as we shall see in the next section.

\subsection{Distributed computing using prefix reduction}

In this section we show how the prefix algorithm we wrote above can be run in a distributed computing context without modification. The key is to make use of overloading using the multimethod dispatch feature of Julia.

Here is a figure summarizing benchmark timings for a sample problem.

Limitations: we specifically took out the overhead needed for distributing the initial data out of the timings, so that we can focus specifically on the time cost of the actual prefix reduction algorithm.

\subsection{Operator overloading for visualization}

We can use the code above to use useful computations but we can also visualize its execution!

We make use of the fact that Julia's arrays provide indexing semantics which can be overloaded for custom types.

We note that code of the form

a[i] = a[j] + a[k]

is desugared into syntax of the form

x = getindex(a, j)
y = getindex(a, k)
z = x+y
setindex!(a, z, i)

There are much more complicated indexing behaviors but we will not touch on them here. CITE array paper.

The nice thing is you can write a custom type that implements getindex and setindex!. You don't have to go crazy and implement ALL the indexing semantics of Julia arrays, just the methods that are relevant to your specific application. In this problem all that is really needed is the indexing semantics of a single element.

Now all we have to do is to introduce a custom type that doesn't really do anything but instead records every getindex and setindex! operation performed on it. To satisfy the minimum requirements of the prefix reduction algorithm all we need at the moment is for getindex to return nothing and to define a dummy method for the associative operator + that operates on nothing. nothing is a value of the special singleton type Void, akin to Python's none or Haskell's Nothing.

Here is a code listing of implementing the trace type

Here is a figure of the code rendered in Compose. Compose is a Julia package for declarative vector graphics 

Here is code that was used to generate the figure.

\section{Related work}

What do other languages do?

Our approach is rather naive and does not account for the complexities in real world implementations, for example possible synchronicity issues produced by higher levels of the broadcast and reduce trees that could result in bus saturation. In principle this can be handled at the scheduler level in the system but we currently don't have the capabilities to do so in Julia.

\section{Conclusions and outlook}

Here is a way to visualize parallel algorithms and study their correctness. First we show that with pure operator polymorphism we can explicitly show the equivalence of a parallel algorithm and a sequential execution that accomplishes the same computations. This can be used to prove the correctness of a parallel algorithm.

Visualization is a byproduct of a correct algorithm. This is a powerful new way to understand how algorithms work.

How about something about fast multipole and variants?

\section{Acknowledgments}
We gratefully acknowledge the Julia community, especially Jake Bolewski, for insightful discussions.

Also funding.

\bibliographystyle{abbrv}
\bibliography{prefix}

\end{document}
